{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseball Savant pitchlog puller\n",
    "\n",
    "This program pulls the pitch information from a mlb approved advanced statistics website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "[WDM] - Cache is valid for [17/06/2020]\n",
      "[WDM] - Looking for [chromedriver 83.0.4103.39 win32] driver in cache \n",
      "[WDM] - Driver found in cache [C:\\Users\\16142\\.wdm\\drivers\\chromedriver\\83.0.4103.39\\win32\\chromedriver.exe]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/119.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\16142\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"https://www.mlbstatic.com/team-logos/117.svg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "#bring in all the libraries I'll need\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "# set driver options and import urls of each page of pitch lists\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(' - incognito')\n",
    "\n",
    "pitch=pd.read_csv('E://HOU18/World series 2017/2017WS_games.csv',usecols=['pitch_url']+['Youtube search'])\n",
    "\n",
    "pitch_url=[]\n",
    "title=[]\n",
    "for i,x in pitch.iterrows():\n",
    "    pitch_url.append(x[0])\n",
    "    title.append(x[1])\n",
    "\n",
    "\n",
    "# game number, count starts at 0\n",
    "j=0\n",
    "while j < 7:\n",
    "    print(j)\n",
    "    #initiate dataframe\n",
    "    df = pd.DataFrame( columns = ['astro_batter', 'pitcher','pitch_type','batter','game_pitch',\n",
    "                                       'pitch_count','plate_appearence','inning','result',\n",
    "                                      'pitch_velo_(mph)','exit_velo_(mph)','launch_angle_(degrees)','distance_(ft)','xBA'])\n",
    "    #initiate row maker\n",
    "    row={}\n",
    "\n",
    "    #initialize driver\n",
    "   \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    time.sleep(4)\n",
    "    driver.implicitly_wait(30)\n",
    "\n",
    "    #go to baseball savant webpage\n",
    "    driver.get(f'{pitch_url[j]}')\n",
    "    time.sleep(4)\n",
    "\n",
    "    #find number of rows will need to be pulled\n",
    "    count=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[1]/td[7]/span')\n",
    "    count=BeautifulSoup(count.text,'html.parser')\n",
    "\n",
    "    #row counter\n",
    "    n=1\n",
    "\n",
    "    #loop through each row of the pitch log\n",
    "    while n<=int(str(count)):\n",
    "\n",
    "        #pull the info I want and parse it\n",
    "\n",
    "        #batter's team, is it an Astro or the other team\n",
    "        astro=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[5]/img')\n",
    "        soup=BeautifulSoup(astro.get_attribute('src'),'html.parser')\n",
    "        team=re.findall('\\d+', str(soup))[0]\n",
    "        if int(team) ==117:\n",
    "            row['astro_batter']=1\n",
    "        else:\n",
    "            row['astro_batter']=0\n",
    "\n",
    "        #Pitcher\n",
    "        pitcher= driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[3]/span')\n",
    "        soup=BeautifulSoup(pitcher.text,'html.parser')\n",
    "        row['pitcher']=str(soup)\n",
    "\n",
    "        #Pitch Type\n",
    "        pitchtype=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[4]/span')\n",
    "        soup=BeautifulSoup(pitchtype.text,'html.parser')\n",
    "        row['pitch_type']=str(soup)\n",
    "\n",
    "        # Batter\n",
    "        batter=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[6]/span')\n",
    "        soup=BeautifulSoup(batter.text,'html.parser')\n",
    "        row['batter']=str(soup)\n",
    "\n",
    "        #Game pitch number\n",
    "        gamepitch=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[7]/span')\n",
    "        soup=BeautifulSoup(gamepitch.text,'html.parser')\n",
    "        row['game_pitch']=int(str(soup))\n",
    "\n",
    "        # pitch number\n",
    "        pitch=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[8]/span')\n",
    "        soup=BeautifulSoup(pitch.text,'html.parser')\n",
    "        row['pitch_count']=int(str(soup))\n",
    "\n",
    "        #plate appearence\n",
    "        plateappearence=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[9]/span')\n",
    "        soup=BeautifulSoup(plateappearence.text,'html.parser')\n",
    "        row['plate_appearence']=int(str(soup))\n",
    "\n",
    "        #inning\n",
    "        inning=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[10]/span')\n",
    "        soup=BeautifulSoup(inning.text,'html.parser')\n",
    "        row['inning']=int(str(soup))\n",
    "\n",
    "        #result\n",
    "        result=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[11]/span')\n",
    "        soup=BeautifulSoup(result.text,'html.parser')\n",
    "        row['result']=str(soup)\n",
    "\n",
    "        #Velo\n",
    "        velo=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[12]/span')\n",
    "        soup=BeautifulSoup(velo.text,'html.parser')\n",
    "        row['pitch_velo_(mph)']=str(soup)\n",
    "\n",
    "        #exit\n",
    "        exit=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[13]/span')\n",
    "        soup=BeautifulSoup(exit.text,'html.parser')\n",
    "        row['exit_velo_(mph)']=str(soup)\n",
    "\n",
    "        #Launch\n",
    "        launch=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[14]/span')\n",
    "        soup=BeautifulSoup(launch.text,'html.parser')\n",
    "        row['launch_angle_(degrees)']=str(soup)\n",
    "\n",
    "        #distance\n",
    "        dist=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[15]/span')\n",
    "        soup=BeautifulSoup(dist.text,'html.parser')\n",
    "        row['distance_(ft)']=str(soup)\n",
    "\n",
    "        #xBA\n",
    "        xba=driver.find_element_by_xpath(f'/html/body/div[3]/div[6]/table/tbody/tr[{n}]/td[16]/span')\n",
    "        soup=BeautifulSoup(xba.text,'html.parser')\n",
    "        row['xBA']=str(soup)\n",
    "\n",
    "        #commit row to dataframe, clear row to start again\n",
    "        df=df.append(row, ignore_index=True)\n",
    "        row={}\n",
    "\n",
    "        #increment\n",
    "        n+=1\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    #close webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    #export collected info\n",
    "    df.sort_values(by=['game_pitch'],inplace=True)\n",
    "    df.set_index('game_pitch',inplace=True)\n",
    "    df.to_csv(f'E://HOU18/World series 2017/games/game {j+1}/savant log.csv',index=False)\n",
    "    \n",
    "    j+=1\n",
    "    \n",
    "print('done!')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
